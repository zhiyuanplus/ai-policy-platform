# 数据处理与分析逻辑说明

本文档详细说明了在AI政策分析工具的数据管道中，所使用的数据处理和量化分析的核心逻辑。

## 1. 数据清洗与筛选 (Data Cleaning & Filtering)

在进入分析阶段之前，所有从各来源抓取到的原始数据都会在 `main.py` 中进行一系列的集中清洗和筛选，以确保数据质量和相关性。

### 1.1 智能去重 (Intelligent De-duplication)

同一个政策事件可能会产生多个相关文件（如“通知”、“正文”、“答记者问”）。为避免重复计算，我们采用了一种智能去重策略：
1.  **提取核心标题**：程序会优先识别并提取每个文档标题中的书名号`《...》`内的文本，作为该政策的“核心标题”。如果标题中没有书名号，则使用完整标题。
2.  **排序**：所有政策按照“核心标题”、“发布日期”（降序）和“内容长度”（降序）进行排序。
3.  **保留最佳版本**：基于“核心标题”进行去重，只保留排序最靠前的一条记录。这确保了我们能为每个政策保留住那个最新、最完整的核心文件。

### 1.2 AI政策筛选 (AI Policy Filtering)

为了从所有政策中筛选出与人工智能强相关的部分，我们设置了基于关键词的评分和筛选机制：
1.  **关键词列表**：我们维护一个与AI强相关的核心关键词列表。
    ```python
    [ "人工智能", "AI", "生成式", "大模型", "AIGC", "算法", "深度合成", "机器学习", "深度学习", "自然语言处理", "智能", "算法推荐" ]
    ```
2.  **计算AI相关度分数 (`ai_score`)**：程序会计算每个政策的标题和正文中出现上述关键词的频率和权重，生成一个“AI相关度分数”。核心术语（如“人工智能”）的权重更高。
3.  **筛选**：只有 `ai_score` **大于4** 的政策才会被认定为AI相关政策，并进入后续的分析环节。这个阈值能有效过滤掉仅宽泛提及“智能”或“算法”的非核心AI政策。

### 1.3 部门名称标准化 (Department Name Standardization)

为了在图表中进行统一归类，程序会将原始的发布部门名称进行标准化处理，例如将“工业和信息化部办公厅”统一为“中华人民共和国工业和信息化部”。处理后的结果存储在`unified_department`列中。

## 2. 监管分数（情绪分析）计算

为了量化每条政策的监管严格程度，`ai_analysis.py`中的`PolicyAnalyzer`类实现了一个`calculate_regulatory_score`函数。该函数通过分析政策文本（标题和正文），计算出一个范围在 **1.0 到 10.0** 之间的分数。

- **1.0** 代表 **极度鼓励创新**
- **10.0** 代表 **极其严格限制**
- **5.0** 代表 **中性**

**计算逻辑：**

1.  **关键词词典**：函数内置了两组关键词词典：
    -   `strict_keywords`: 包含具有限制、禁止、惩罚等含义的词语（如：“禁止”, “处罚”, “不得”）。
    -   `innovation_keywords`: 包含具有鼓励、支持、发展等含义的词语（如：“鼓励”, “创新”, “支持”）。
2.  **词频加权**：函数会统计每个关键词在文本中出现的次数，并根据词频对每个词的预设分数进行加权。
3.  **加权平均与归一化**：最终通过加权平均计算得到一个原始分数，并将其归一化到 1.0 到 10.0 的区间内。如果未找到任何关键词，则返回中性分 5.0。

这个分数作为一个关键指标，用于后续的趋势分析、风险预警和仪表盘可视化。