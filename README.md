# AI æ”¿ç­–æƒ…æŠ¥å¹³å° (AIRP)

AI Regulatory & Policy (AIRP) Platform - An automated platform for tracking, analyzing, and visualizing AI-related government policies.

## åŠŸèƒ½æ¦‚è§ˆ (Features)

This project implements a complete pipeline for monitoring and analyzing AI policies, focusing on sources from China.

### ğŸ” æ•°æ®é‡‡é›† (Data Scraping)
- Scrapers for three major Chinese regulatory bodies:
  - **MIIT (å·¥ä¿¡éƒ¨)**: `miit_scraper.py`
  - **CAC (ç½‘ä¿¡åŠ)**: `cac_scraper_v3.py`
  - **TC260 (å…¨å›½ä¿¡å®‰æ ‡å§”)**: `tc260_scraper.py`
- The scrapers are designed to fetch all recent documents, with AI-related filtering handled centrally.

### âš™ï¸ æ•°æ®å¤„ç†ä¸åˆ†æ (Data Processing & Analysis)
- All processing logic is centralized in `main.py` for efficiency and consistency.
- **æ™ºèƒ½å»é‡ (Intelligent De-duplication)**: Identifies and consolidates multiple documents (e.g., announcements, full texts, Q&As) related to the same core policy into a single entry.
- **AIæ”¿ç­–ç­›é€‰ (AI Policy Filtering)**: A scoring mechanism (`ai_score > 4`) filters for policies that are highly relevant to Artificial Intelligence, using a refined list of keywords.
- **å¤šç»´åº¦é‡åŒ– (Multi-dimensional Quantification)**: The `ai_analysis.py` module enriches each policy with:
  - **ç›‘ç®¡æ€åº¦è¯„åˆ† (Regulatory Score)**: A 1-10 score indicating the government's stance (innovation vs. regulation).
  - **æ¶‰åŠé¢†åŸŸ (Identified Domains)**: Tags like "Data Security", "Generative AI", etc.
  - **æ³•å¾‹æ•ˆåŠ›å±‚çº§ (Enforcement Level)**: Classifies documents into categories like "Laws & Regulations", "Administrative Rules", etc.

### ğŸ“Š äº¤äº’å¼çœ‹æ¿ (Interactive Dashboard)
- A web-based dashboard built with Streamlit (`dashboard.py`).
- **å¤šè¯­è¨€æ”¯æŒ (Multi-language)**: Fully bilingual interface (Chinese/English) with a real-time language switcher.
- **æ ¸å¿ƒå›¾è¡¨ (Core Visualizations)**:
  - **ç›‘ç®¡æƒ…ç»ªèµ°åŠ¿å›¾ (Regulatory Sentiment Trend)**: Tracks the average regulatory score over time (Year/Quarter/Month).
  - **å„éƒ¨å§”å‘å¸ƒæƒé‡ (Policy Distribution by Department)**: A pie chart showing the proportion of policies from each government body.
  - **æ”¿ç­–æ³•å¾‹æ•ˆåŠ›å±‚çº§ (Policy Legal Force Level)**: A bar chart showing the distribution of policies by their legal authority.
- **æ”¿ç­–æ•°æ®è¯¦æƒ… (Policy Data Details)**: A detailed, sortable, and searchable table of all filtered policies, with automatic text wrapping for long titles.
- **æ³•å¾‹æ³•è§„å‚è€ƒ (Legal Reference)**: A quick reference section with links to the full text of key AI-related laws and regulations in China.

## å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. ç¯å¢ƒå‡†å¤‡ (Prerequisites)
Ensure you have Python 3 installed. Then, install the required packages:
```bash
pip3 install -r requirements.txt
```

### 2. è¿è¡Œæµç¨‹ (Workflow)

The workflow is designed to be run sequentially.

**Step 1: (Optional) Scrape for New Data**
If you need to fetch the latest policies, run the scraper scripts. Each script saves its findings to a `*_all_policies.csv` file.
```bash
python3 cac_scraper_v3.py
python3 miit_scraper.py
python3 tc260_scraper.py
```

**Step 2: Process, Filter, and Analyze Data**
This is a mandatory step. It reads all `*_all_policies.csv` files, performs de-duplication and AI filtering, runs the analysis, and saves the final, clean data to `all_policies_analyzed.csv`.
```bash
python3 main.py
```

**Step 3: View the Dashboard**
Launch the interactive web dashboard.
```bash
python3 -m streamlit run dashboard.py
```

## é¡¹ç›®ç»“æ„ (Project Structure)

```
AI_Policy_Tool/
â”œâ”€â”€ main.py                    # Main data processing and analysis pipeline
â”œâ”€â”€ cac_scraper_v3.py          # Scraper for CAC
â”œâ”€â”€ miit_scraper.py            # Scraper for MIIT
â”œâ”€â”€ tc260_scraper.py           # Scraper for TC260
â”œâ”€â”€ ai_analysis.py             # AI analysis and quantification module
â”œâ”€â”€ dashboard.py               # Streamlit interactive dashboard
â”œâ”€â”€ AIPolicyTool_PRD.md        # Product Requirements Document
â”œâ”€â”€ DATA_PROCESSING_LOGIC.md   # Details on data cleaning and analysis logic
â”œâ”€â”€ requirements.txt           # Python package dependencies
â””â”€â”€ README.md                  # This file
```

## è¾“å‡ºæ–‡ä»¶ (Output Files)

- `*_all_policies.csv`: Raw, unfiltered data scraped from each source.
- `all_policies_analyzed.csv`: The final, cleaned, de-duplicated, filtered, and analyzed data ready for the dashboard.
- `metadata.json`: Contains metadata, such as the latest date across all scraped policies.
